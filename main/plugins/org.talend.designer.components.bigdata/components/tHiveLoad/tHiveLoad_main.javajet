<%@ jet
imports="
        org.talend.core.model.process.INode
        org.talend.core.model.process.ElementParameterParser
        org.talend.core.model.metadata.IMetadataTable
        org.talend.core.model.metadata.IMetadataColumn
        org.talend.designer.codegen.config.CodeGeneratorArgument
        org.talend.core.model.process.IConnection
        org.talend.core.model.process.IConnectionCategory
        org.talend.core.model.metadata.MappingTypeRetriever
        org.talend.core.model.metadata.MetadataTalendType
        org.talend.designer.runprocess.ProcessorException
        org.talend.designer.runprocess.ProcessorUtilities
        java.util.List
        java.util.Map
        java.util.Set
        java.util.HashSet
        "
%>
<%@ include file="@{org.talend.designer.components.localprovider}/components/templates/Log4j/Log4jDBConnUtil.javajet"%>

<%
    CodeGeneratorArgument codeGenArgument = (CodeGeneratorArgument) argument;
    INode node = (INode)codeGenArgument.getArgument();
    final String cid = node.getUniqueName();
    String processId = node.getProcess().getId();

    String dbhost = ElementParameterParser.getValue(node, "__HOST__");
    String dbport = ElementParameterParser.getValue(node, "__PORT__");
    String dbname= ElementParameterParser.getValue(node, "__DBNAME__");
    String dbuser= ElementParameterParser.getValue(node, "__USER__");

    boolean isLog4jEnabled = ("true").equals(ElementParameterParser.getValue(node.getProcess(), "__LOG4J_ACTIVATE__"));
    String theDistribution = ElementParameterParser.getValue(node, "__DISTRIBUTION__");
    String theVersion = ElementParameterParser.getValue(node, "__HIVE_VERSION__");

	final String studioVersion = org.talend.commons.utils.VersionUtils.getDisplayVersion();

	String encryptedToken = null;
    if("true".equals(ElementParameterParser.getValue(node,"__USE_EXISTING_CONNECTION__"))) {
        String connection = ElementParameterParser.getValue(node, "__CONNECTION__");
        for (INode pNode : node.getProcess().getNodesOfType("tHiveConnection")) {
            if(connection!=null && connection.equals(pNode.getUniqueName())) {
                theDistribution = ElementParameterParser.getValue(pNode, "__DISTRIBUTION__");
                theVersion = ElementParameterParser.getValue(pNode, "__HIVE_VERSION__");
            }
        }
    }

    org.talend.hadoop.distribution.component.HiveComponent hiveDistrib = null;
    try {
        hiveDistrib = (org.talend.hadoop.distribution.component.HiveComponent) org.talend.hadoop.distribution.DistributionFactory.buildDistribution(theDistribution, theVersion);
    } catch (java.lang.Exception e) {
        e.printStackTrace();
        return "";
    }
    boolean isCustom = hiveDistrib instanceof org.talend.hadoop.distribution.custom.CustomDistribution;

    if(hiveDistrib.isExecutedThroughWebHCat()) {
%>
        <%@ include file="@{org.talend.designer.components.localprovider}/components/templates/Hive/GetAzureConnection.javajet"%>
<%
        if("false".equals(useExistingConn)) { // This variable is declared and initialized in the GetAzureConnection.javajet
            boolean setMemory = "true".equals(ElementParameterParser.getValue(node, "__SET_MEMORY__"));
            if(setMemory) {
                String mapMemory = ElementParameterParser.getValue(node,"__MAPREDUCE_MAP_MEMORY_MB__");
                String reduceMemory = ElementParameterParser.getValue(node,"__MAPREDUCE_REDUCE_MEMORY_MB__");
                String amMemory = ElementParameterParser.getValue(node,"__YARN_APP_MAPREDUCE_AM_RESOURCE_MB__");
%>
                bw_<%=cid%>.write("SET mapreduce.map.memory.mb=" + <%=mapMemory%> + ";");
                bw_<%=cid%>.write("SET mapreduce.reduce.memory.mb=" + <%=reduceMemory%> + ";");
                bw_<%=cid%>.write("SET yarn.app.mapreduce.am.resource.mb=" + <%=amMemory%> + ";");
<%
            }

            List<Map<String, String>> advProps = (List<Map<String,String>>)ElementParameterParser.getObjectValue(node, "__ADVANCED_PROPERTIES__");
            if(advProps!=null) {
                for(Map<String, String> item : advProps){
%>
                    bw_<%=cid%>.write("SET "+<%=item.get("PROPERTY")%>+"="+<%=item.get("VALUE")%> + ";");
<%
                }
            }
%>
            String dbname_<%=cid%> = <%=dbname%>;
            if(dbname_<%=cid%>!=null && !"".equals(dbname_<%=cid%>.trim()) && !"default".equals(dbname_<%=cid%>.trim())) {
                bw_<%=cid%>.write("use " + dbname_<%=cid%> + ";");
            }
<%
        }
    } else if(hiveDistrib.doSupportUniversalDataprocMode()) {
%>
        <%@ include file="@{org.talend.designer.components.localprovider}/components/templates/Hive/GetDataprocConnection.javajet"%>
<%
	} else {
%>
        <%@ include file="@{org.talend.designer.components.localprovider}/components/templates/Hive/GetConnection.javajet"%>
<%
    }

    String useExistingConn = ElementParameterParser.getValue(node,"__USE_EXISTING_CONNECTION__");

    String loadAction = ElementParameterParser.getValue(node, "__LOAD_ACTION__");
    boolean local = "true".equals(ElementParameterParser.getValue(node, "__LOCAL__"));
    String path = ElementParameterParser.getValue(node, "__FILEPATH__");
    String fileAction = ElementParameterParser.getValue(node, "__FILE_ACTION__");
    String tablename = ElementParameterParser.getValue(node, "__TABLE__");
    boolean setPartition = "true".equals(ElementParameterParser.getValue(node, "__SET_PARTITIONS__"));
    String partition = ElementParameterParser.getValue(node, "__PARTITION__");
    String dieOnError = ElementParameterParser.getValue(node, "__DIE_ON_ERROR__");

    boolean useExistingConnection = "true".equals(ElementParameterParser.getValue(node,"__USE_EXISTING_CONNECTION__"));

    String connectionMode = ElementParameterParser.getValue(node, "__CONNECTION_MODE__");
    boolean setFsDefaultName = "true".equals(ElementParameterParser.getValue(node, "__SET_FS_DEFAULT_NAME__"));
    String fsDefaultName = ElementParameterParser.getValue(node, "__FS_DEFAULT_NAME__");
    INode connectionInformationNode = node;

    if(useExistingConnection && !hiveDistrib.useCloudLauncher()) {
        setFsDefaultName = false;
        fsDefaultName = "";
        dbuser = "";
        String connection = ElementParameterParser.getValue(node, "__CONNECTION__");
        for (INode pNode : node.getProcess().getNodesOfType("tHiveConnection")) {
            if(connection!=null && connection.equals(pNode.getUniqueName())) {
                connectionMode = ElementParameterParser.getValue(pNode, "__CONNECTION_MODE__");
                setFsDefaultName = "true".equals(ElementParameterParser.getValue(pNode, "__SET_FS_DEFAULT_NAME__"));
                fsDefaultName = ElementParameterParser.getValue(pNode, "__FS_DEFAULT_NAME__");
                dbuser = ElementParameterParser.getValue(pNode, "__USER__");
                connectionInformationNode = pNode;
                break;
            }
        }
    }

    // Register jars to handle the parquet format.

    boolean targetTableUsesParquetFormat = "true".equals(ElementParameterParser.getValue(node, "__TARGET_TABLE_IS_A_PARQUET_TABLE__"));

    boolean isParquetSupported = isCustom || hiveDistrib.doSupportParquetFormat();
    if(targetTableUsesParquetFormat && !isParquetSupported) {
%>
        if(true) {
            throw new java.lang.UnsupportedOperationException("Parquet is only supported if the distribution uses embedded Hive version 0.10 or later.");
        }
<%
    }

    boolean generateAddJarCodeForAll = targetTableUsesParquetFormat;

    if(targetTableUsesParquetFormat) {
        String compression = ElementParameterParser.getValue(node, "__PARQUET_COMPRESSION__");
        java.util.List<String> jarsToRegister = null;
        java.util.List<String> jars = null;
        if(generateAddJarCodeForAll) {
            String[] commandLine = new String[] {"<command>"};
            try {
                commandLine = ProcessorUtilities.getCommandLine("win32",true, processId, "",org.talend.designer.runprocess.IProcessor.NO_STATISTICS,org.talend.designer.runprocess.IProcessor.NO_TRACES, new String[]{});
            } catch (ProcessorException e) {
                e.printStackTrace();
            }

            jarsToRegister = new java.util.ArrayList();

            jarsToRegister.add("snappy-java");
            jarsToRegister.add("parquet-hive-bundle");
            //jarsToRegister.add("parquet-hadoop-bundle");

            for (int j = 0; j < commandLine.length; j++) {
                if(commandLine[j].contains("jar")) {
                    jars = java.util.Arrays.asList(commandLine[j].split(";"));
                    break;
                }
            }
        }

        if(jarsToRegister!=null && jars!=null) {
            if("EMBEDDED".equalsIgnoreCase(connectionMode) || hiveDistrib.useCloudLauncher()) {
%>
                routines.system.GetJarsToRegister getJarsToRegister_<%=cid %> = new routines.system.GetJarsToRegister();
<%
            } else {
                generateAddJarCodeForAll = false;
                if(setFsDefaultName) {
                    generateAddJarCodeForAll = true;
%>
                    <%@ include file="@{org.talend.designer.components.localprovider}/components/templates/Hive/GetHiveJarsToRegister.javajet"%>
                    GetHiveJarsToRegister_<%=cid%> getJarsToRegister_<%=cid %> = new GetHiveJarsToRegister_<%=cid%>();
<%
                }
            }

            if(generateAddJarCodeForAll) {
                if(!hiveDistrib.useCloudLauncher()) { // Then we create a SQL statement to add the jars.
%>
                    java.sql.Statement addJar_<%=cid%> = null;
<%
                }
                for(int i=0; i<jarsToRegister.size(); i++) {
                    String jarToRegister = jarsToRegister.get(i);
                    for(int j=0; j<jars.size(); j++) {
                        if(jars.get(j).contains(jarToRegister)) {
                            if(!hiveDistrib.useCloudLauncher()) { // Then we use the created SQL statement to add the jars.
%>
                                addJar_<%=cid%> = conn_<%=cid%>.createStatement();
                                try {
                                    addJar_<%=cid%>.execute("add jar " + getJarsToRegister_<%=cid %>.replaceJarPaths("<%=jars.get(j)%>"));
                                } catch (Exception e) {
                                    e.printStackTrace();
                                } finally {
                                    addJar_<%=cid%>.close();
                                }
<%
                            } else { // cloud distribution
                                if(hiveDistrib.isExecutedThroughWebHCat()) {
%>
                                    bw_<%=cid%>.write("ADD JAR " + wasbPath_<%=cid%> + new java.io.File(getJarsToRegister_<%=cid %>.replaceJarPaths("<%=jars.get(j)%>")).getName() + ";");
                                    libjars_<%=cid%>.append(getJarsToRegister_<%=cid %>.replaceJarPaths("<%=jars.get(j)%>") + ",");
<%
                            	} else { // dataproc
                                    if(isLog4jEnabled) {
%>
                                        log.debug("Query for <%=cid%>: " + "ADD JAR " + stagingBucketPath_<%=cid%> + new java.io.File(getJarsToRegister_<%=cid %>.replaceJarPaths("<%=jars.get(j)%>")).getName() + ";");
<%
                                    }
%>
                                    instance_<%=cid%>.addQuery("ADD JAR " + stagingBucketPath_<%=cid%> + new java.io.File(getJarsToRegister_<%=cid %>.replaceJarPaths("<%=jars.get(j)%>")).getName() + ";");
                                    libjars_<%=cid%>.append(getJarsToRegister_<%=cid %>.replaceJarPaths("<%=jars.get(j)%>") + ",");
<%
                                }
                            }
                        }
                    }
                }
            }
        }

        // End of parquet format handling.

        if(!hiveDistrib.useCloudLauncher()) {
%>
            java.sql.Statement setCompression_<%=cid%> = conn_<%=cid%>.createStatement();
            try {
                setCompression_<%=cid%>.execute("SET parquet.compression=<%=compression%>");
            } finally {
                setCompression_<%=cid%>.close();
            }
<%
        } else {
            if(hiveDistrib.isExecutedThroughWebHCat()) {
%>
                bw_<%=cid%>.write("SET parquet.compression=<%=compression%>;");
<%
            } else { // dataproc
                if(isLog4jEnabled) {
%>
                    log.debug("Query for <%=cid%>: " + "SET parquet.compression=<%=compression%>;");
<%
                }
%>
                instance_<%=cid%>.addQuery("SET parquet.compression=<%=compression%>;");
<%                
            }
        }
    }

%>
    <%@ include file="@{org.talend.designer.components.bigdata}/components/tHiveLoad/loadQueryGeneration.javajet"%>
	<%
    boolean setApplicationName=ElementParameterParser.getBooleanValue(node, "__SET_APPLICATION_NAME__");
    if(!hiveDistrib.useCloudLauncher() && !hiveDistrib.doSupportUniversalDataprocMode()) {
    %>
        java.sql.Statement stmt_<%=cid %> = conn_<%=cid %>.createStatement();
        try {
			<%@ include file="@{org.talend.designer.components.localprovider}/components/templates/Hive/SetQueryName.javajet"%>
            stmt_<%=cid%>.execute(querySQL_<%=cid %>);
        } catch(java.sql.SQLException e_<%=cid%>) {
            <%if(("true").equals(dieOnError)) {
            %>
                throw(e_<%=cid%>);
            <%
            }else {
            %>
                System.err.println(e_<%=cid%>.getMessage());
            <%
            }%>
        }
        stmt_<%=cid %>.close();
        <%if(!("true").equals(useExistingConn)) {%>
            conn_<%=cid %>.close();
        <%}%>
        globalMap.put("<%=cid%>_QUERY", querySQL_<%=cid %>);

        String currentClientPathSeparator_<%=cid%> = (String)globalMap.get("current_client_path_separator");
        if(currentClientPathSeparator_<%=cid%>!=null) {
            System.setProperty("path.separator", currentClientPathSeparator_<%=cid%>);
            globalMap.put("current_client_path_separator", null);
        }

        String currentClientUsername_<%=cid%> = (String)globalMap.get("current_client_user_name");
        if(currentClientUsername_<%=cid%>!=null) {
            System.setProperty("user.name", currentClientUsername_<%=cid%>);
            globalMap.put("current_client_user_name", null);
        }

        String originalHadoopUsername_<%=cid%> = (String)globalMap.get("HADOOP_USER_NAME_<%=cid%>");
        if(originalHadoopUsername_<%=cid%>!=null) {
            System.setProperty("HADOOP_USER_NAME", originalHadoopUsername_<%=cid%>);
            globalMap.put("HADOOP_USER_NAME_<%=cid%>", null);
        } else {
            System.clearProperty("HADOOP_USER_NAME");
        }
<%
    } else { // cloud distribution
        if(hiveDistrib.isExecutedThroughWebHCat()) {
%>
            bw_<%=cid%>.write(querySQL_<%=cid %> + ";");
            globalMap.put("<%=cid%>_QUERY", querySQL_<%=cid %>);
    
            bw_<%=cid%>.close();
    
            if(libjars_<%=cid%>.length() > 0) {
                instance_<%=cid%>.setLibJars(libjars_<%=cid%>.toString().substring(0, libjars_<%=cid%>.length()-1));
            }
            instance_<%=cid%>.callWS(instance_<%=cid%>.sendFiles());
            int exitCode_<%=cid%> = instance_<%=cid%>.execute();
            if(exitCode_<%=cid%> > 0) {
    
<%
                if(("true").equals(dieOnError)) {
%>
                    throw new Exception("The Hive job failed. Please read the logs for more details");
<%
                } else {
%>
                    System.err.println("The Hive job failed. Please read the logs for more details");
<%
                    if(isLog4jEnabled) {
%>
                        log.error("<%=cid%> - The Hive job failed. Please read the logs for more details");
<%
                    }
                }
%>
            }
<%
        } else { // dataproc
            if(isLog4jEnabled) {
%>
                log.debug("Query for <%=cid%>: " + querySQL_<%=cid %>.replace("';'", "'\\;'") + ";");
<%
            }
%>
            instance_<%=cid%>.addQuery(querySQL_<%=cid %>.replace("';'", "'\\;'") + ";");
            globalMap.put("<%=cid%>_QUERY", querySQL_<%=cid %>);
            int exitCode_<%=cid%> = instance_<%=cid%>.executeJob();
            System.out.println(instance_<%=cid%>.getJobLog());
            if(exitCode_<%=cid%> > 0) {
    
<%
                if(("true").equals(dieOnError)) {
%>
                    throw new Exception("The Hive job failed. Please read the logs for more details");
<%
                } else {
%>
                    System.err.println("The Hive job failed. Please read the logs for more details");
<%
                    if(isLog4jEnabled) {
%>
                        log.error("<%=cid%> - The Hive job failed. Please read the logs for more details");
<%
                    }
                }
%>
            }
<%
        } // else part of if(hiveDistrib.isExecutedThroughWebHCat()) 
    } // else port of useCloudLauncher
%>